ğŸ“˜ Renewable Energy Generation Analytics â€“ Azure Databricks Medallion Architecture
ğŸš€ Project Overview

El presente proyecto tiene como objetivo analizar el desempeÃ±o operativo de activos de generaciÃ³n de energÃ­a renovable, integrando datos de producciÃ³n y variables hidrolÃ³gicas para construir indicadores clave de desempeÃ±o (KPIs) orientados a la toma de decisiones.

A partir de datasets estructurados en formato CSV almacenados en un Data Lake en Azure, se busca:

Evaluar la producciÃ³n energÃ©tica por central

Analizar la disponibilidad operativa

Calcular el factor de planta (Capacity Factor)

Medir el impacto de indisponibilidades

Integrar variables hidrolÃ³gicas relevantes

Construir una capa analÃ­tica lista para dashboards ejecutivos

Para lograrlo, se implementa una arquitectura Medallion en Azure Databricks que permite estructurar el flujo de datos desde su estado crudo hasta su forma analÃ­tica optimizada, garantizando gobernanza, trazabilidad y escalabilidad.

El resultado final no es Ãºnicamente un ETL, sino una plataforma analÃ­tica estructurada y gobernada para el monitoreo del desempeÃ±o energÃ©tico.

ğŸ— Arquitectura General
Arquitectura Medallion

La soluciÃ³n se basa en el patrÃ³n Medallion (Bronze â†’ Silver â†’ Golden), bajo Unity Catalog y orquestado mediante Databricks Workflows.

ğŸ”¹ Capa RAW (Azure Data Lake Gen2)

Fuente: Archivos CSV

AutenticaciÃ³n: Managed Identity

UbicaciÃ³n: Azure Data Lake Storage Gen2

RestricciÃ³n del proyecto:

âŒ No se utiliza DBFS como raw

âŒ No se utilizan Volumes como raw

Esta capa contiene la informaciÃ³n original sin transformaciÃ³n, asegurando integridad y trazabilidad.

ğŸ¥‰ Bronze â€“ Extract

Objetivo: Preservar la data en su estado mÃ¡s cercano al origen.

CaracterÃ­sticas:

Lectura directa desde ADLS

Escritura en formato Delta

InclusiÃ³n de metadatos:

ingestion_timestamp

source_file

environment

En esta capa no se aplican reglas de negocio complejas; solo estructuraciÃ³n bÃ¡sica y persistencia confiable.

ğŸ¥ˆ Silver â€“ Transform

Objetivo: Mejorar la calidad y consistencia de los datos.

Transformaciones realizadas:

Cast de tipos de datos

Limpieza de valores nulos

EliminaciÃ³n de duplicados

Validaciones de reglas de negocio

EstandarizaciÃ³n de columnas

IntegraciÃ³n entre datasets (producciÃ³n + hidrologÃ­a)

Esta capa representa datos estructurados y confiables listos para anÃ¡lisis.

ğŸ¥‡ Golden â€“ Load (Capa AnalÃ­tica)

Objetivo: Generar tablas optimizadas para consumo analÃ­tico y visualizaciÃ³n.

Se construyen:

Agregaciones por central

KPIs de desempeÃ±o

Indicadores de disponibilidad

Factor de planta

Impacto de indisponibilidades

MÃ©tricas consolidadas por periodo

Se aplican optimizaciones:

OPTIMIZE

ZORDER

Tablas Delta optimizadas para consulta

Esta capa alimenta directamente los dashboards ejecutivos.

âš™ï¸ Workflow Implementado

El pipeline es orquestado mediante un Databricks Job estructurado de la siguiente manera:

1ï¸âƒ£ PrepAmb

PreparaciÃ³n del entorno

ValidaciÃ³n de catÃ¡logos y esquemas

InicializaciÃ³n de variables

2ï¸âƒ£ Ingesta Paralela (Bronze)

ingest_generation

ingest_hydrology

Permite cargar mÃºltiples fuentes de manera concurrente, mejorando eficiencia.

3ï¸âƒ£ Transform

Procesa Bronze â†’ Silver

Aplica reglas de negocio

Integra datasets

4ï¸âƒ£ Load

Genera capa Golden

Calcula KPIs

Aplica optimizaciÃ³n Delta

5ï¸âƒ£ Grants

Asigna permisos en Unity Catalog

Control de acceso por roles:

DataEngineers

DataScientists

Este flujo garantiza:

OrquestaciÃ³n controlada

SeparaciÃ³n clara de responsabilidades

Gobernanza estructurada

ğŸ” Seguridad y Gobernanza

Acceso a ADLS exclusivamente mediante Managed Identity

Unity Catalog para:

Aislamiento de entornos

Control de acceso basado en roles

Gobernanza centralizada

SeparaciÃ³n entre entorno Dev y Prod

Sin credenciales embebidas en cÃ³digo

ğŸŒ Entornos
Desarrollo

Rama: develop

Workspace: Dev

Uso: pruebas y validaciones

ProducciÃ³n

Rama: main

Workspace: Prod

Despliegue vÃ­a CI/CD

ğŸ”„ CI/CD â€“ GitHub Actions

El proyecto integra un pipeline automatizado que:

Valida cambios en Pull Requests

Despliega notebooks automÃ¡ticamente

Actualiza Workflows

Permite promociÃ³n controlada a producciÃ³n

Esto asegura:

Versionamiento formal

Reproducibilidad

Control de cambios

Buenas prÃ¡cticas DevOps

ğŸ“Š Dashboard Final

Las tablas Golden alimentan dashboards en Databricks con:

ProducciÃ³n total por central

Tendencia del factor de planta

AnÃ¡lisis de indisponibilidades

Comparativos por periodo

KPIs ejecutivos consolidados

La capa Golden estÃ¡ optimizada para consultas analÃ­ticas de alto rendimiento.

ğŸ“ Estructura del Repositorio
project-root/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Preparacion_ambiente.py
â”‚   â”œâ”€â”€ Ingest_hydro_generation_bronze.py
â”‚   â”œâ”€â”€ Ingest_hydrology_bronze.py
â”‚   â”œâ”€â”€ Transform.py
â”‚   â”œâ”€â”€ Load.py
â”‚   â””â”€â”€ Grants.py
â”‚
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ etl_workflow.json
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ deploy.yml
â”‚
â”œâ”€â”€ Medallion_architecture_FinalProject.png
â”œâ”€â”€ Workflow_completed.png
â””â”€â”€ README.md
ğŸ¯ Cumplimiento del Proyecto Final

El proyecto cumple con:

Uso de arquitectura Medallion

ETL completo en PySpark

Uso obligatorio de Managed Identity

IntegraciÃ³n de mÃ­nimo dos datasets

Gobernanza mediante Unity Catalog

IntegraciÃ³n CI/CD

VisualizaciÃ³n final

ğŸ EjecuciÃ³n del Proyecto

Cargar archivos CSV en el contenedor RAW de ADLS

Validar permisos de Managed Identity

Ejecutar Workflow en entorno Dev

Validar tablas Silver y Golden

Promover a ProducciÃ³n mediante merge a main

Visualizar dashboard en Databricks
